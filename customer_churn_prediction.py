# -*- coding: utf-8 -*-
"""customer churn prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_LKse0iekcbTGJJ8dC_wLvztNWC82zj8

**Customer Churn Prediction**
"""

#import all the required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/Churn_Modelling.csv')

df

# summary of the dataset
df.describe()

# see dtypes of each column
df.info()

"""Checking and filling missing and Nan values"""

# check for missing values
df.isnull().sum()

df['Geography'].fillna(df['Geography'].mode()[0], inplace=True)
df['Age'].fillna(df['Age'].mean(), inplace=True)
df['HasCrCard'].fillna(df['HasCrCard'].mode()[0], inplace=True)
df['IsActiveMember'].fillna(df['IsActiveMember'].mode()[0], inplace=True)

df.isnull().sum()

# display rows with Nan values
nan = df[df.isnull().any(axis=1)]
print(nan)

# check Nan count
nan_count = df.isna().sum()
print(nan_count)

# check Nan in a particular column
na = df['HasCrCard'].isna().sum()
print(na)

df.info()

df.head(5)

"""Change object type to numeric data - using one-hot encoding or label encoding"""

# using label-encoding - on surname
from sklearn.preprocessing import LabelEncoder

label = LabelEncoder()

df['Surname'] = label.fit_transform(df['Surname'])

# using one-hot encoding - on gender, geography
from sklearn.preprocessing import OneHotEncoder

df = pd.get_dummies(df, columns=['Geography', 'Gender'], drop_first=True)

#convert all boolean types into binary
df[df.select_dtypes(include=['bool']).columns] = df.select_dtypes(include=['bool']).astype(int)

df.shape

"""Surname is usually not useful for prediction because it doesn’t provide any relevant information about customer behavior or churn. It's generally safe to exclude this feature.

"""

df = df.drop('Surname', axis=1)

df.shape

df.columns

# check for Duplicates
duplicates = df.duplicated().sum()
print(duplicates)

#remove duplicates
df.drop_duplicates(inplace=True)

"""Check for Outleirs - IQR and Z-Score"""

# Method 1: Interquartile Range (IQR)
import pandas as pd

# Calculate Q1 (25th percentile) and Q3 (75th percentile)
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)

# Calculate IQR
IQR = Q3 - Q1

# Define outlier condition
outlier_condition = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))

# Count the number of outliers in each numerical column
outliers_count = outlier_condition.sum()
outliers_count

# Method 2: Z-score
from scipy import stats

#numerical columns
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns

# Calculate Z-scores
z_scores = stats.zscore(df[numerical_cols])

# Define outlier condition
outliers = (abs(z_scores) > 3)

# Count the number of outliers in each numerical column
outliers_count_z = outliers.sum(axis=0)
outliers_count_z

# Check the structure of the DataFrame
print(df.info())

# Check the numerical columns
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
print("Numerical Columns:", numerical_cols)

from scipy import stats

# Calculate Z-scores for the numerical columns
z_scores = stats.zscore(df[numerical_cols])

# Convert z_scores to DataFrame for easier manipulation
z_scores_df = pd.DataFrame(z_scores, columns=numerical_cols)

# Check the first few rows of z_scores_df
print(z_scores_df.head())

# Define a threshold for identifying outliers
threshold = 3

# Replace outliers with the mean value of the respective column
for column in numerical_cols:
    mean_value = df[column].mean()

    # Create a boolean mask for outliers
    outlier_mask = abs(z_scores_df[column]) > threshold

    # Replace outliers in the original DataFrame with mean_value
    df.loc[outlier_mask, column] = mean_value

# Check the resulting DataFrame
print(df.describe())

"""Visualize the data"""

df.corr()
sns.heatmap(df.corr())

ssns.histplot(df['Age'])

sns.histplot(df['NumOfProducts'])

df.skew()

# Histogram
import matplotlib.pyplot as plt
import seaborn as sns

# Plot histograms for numerical columns
df[numerical_cols].hist(figsize=(12, 8), bins=30, edgecolor='black')
plt.suptitle("Histograms of Numerical Columns", fontsize=14)
plt.show()

# kDE plot
# KDE plots for numerical columns
plt.figure(figsize=(12, 8))
for column in numerical_cols:
    sns.kdeplot(df[column], label=column)
plt.title("KDE Plot of Numerical Columns")
plt.legend()
plt.show()

# box_plots - Boxplots help visualize outliers and the distribution of data.
df.boxplot(rot=45)
plt.title("Boxplot of Numerical Columns")
plt.show()

# Q-Q Plot - A Q-Q plot compares the data distribution to a normal distribution. If the points follow a straight diagonal line, the data is approximately normal.
import scipy.stats as stats

plt.figure(figsize=(12, 8))
for i, column in enumerate(numerical_cols, 1):
    plt.subplot(3, 3, i)
    stats.probplot(df[column], dist="norm", plot=plt)
    plt.title(f'Q-Q Plot of {column}')

plt.tight_layout()
plt.show()

# Skewness & Kurtosis - Numerical measures like skewness and kurtosis give an indication of whether the data follows a normal distribution.
df_skew_kurt = df.agg(['skew', 'kurtosis']).T
print(df_skew_kurt)

"""Skewness: Measures asymmetry of the distribution.
* If it's close to 0, the data is symmetric.
* Positive (> 0): Right-skewed (long right tail)
* Negative (< 0): Left-skewed (long left tail)

Kurtosis: Measures how "tailed" the distribution is.
* Normal distribution = 3 (excess kurtosis = 0)
* Higher (> 3): More outliers (heavy tails)
* Lower (< 3): Fewer outliers (light tails)

**Data is not normally distributed**
we can apply transformations like -

Log Transformation: df[column] = np.log1p(df[column])

Box-Cox Transformation: df[column], _ = stats.boxcox(df[column]) (only for positive values)

Yeo-Johnson Transformation (for data with negative values): stats.yeojohnson(df[column])

1. Log Transformation → Useful for right-skewed (positive skew) data.
2. Square Root Transformation → Works well for moderate skewness.
3. Box-Cox Transformation → More flexible, works for various skewness levels (requires positive values).
4. Yeo-Johnson Transformation → Similar to Box-Cox but works with zero and negative values.
"""

from sklearn.preprocessing import PowerTransformer

# Apply Yeo-Johnson transformation
pt = PowerTransformer(method='yeo-johnson')
df_transformed = df.copy()
df_transformed[df.select_dtypes(include=[np.number]).columns] = pt.fit_transform(df.select_dtypes(include=[np.number]))

# Check new skewness
df_transformed.skew()

df

x = df.drop('Exited', axis=1)
x.columns

y = df['Exited']

"""**Feature Scaling - Standardscaler**"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

x_scaled = scaler.fit_transform(x)

#put the scaled x into a dataframe
df = pd.DataFrame(x_scaled, columns=x.columns)

#add y into the dataframe
df['Exited'] = y.values

df.head(5)

df['Exited'].value_counts()

l = df.drop('Exited', axis=1)
l.shape

m = df['Exited']
m.shape

# do oversample for minority class
from imblearn.over_sampling import SMOTE
#define oversampling
oversampling = SMOTE(sampling_strategy={1:7000}, random_state=42)
x_over, y_over = oversampling.fit_resample(l,m)
print(f"shape of x is {x_over.shape}, shape of y is {y_over.shape}")

#put in dataframes
df = pd.DataFrame(x_over, columns=x.columns)
df['Exited'] = y_over.values

df['Exited'].value_counts()

x = df.drop('Exited', axis=1)
x.columns

y=df['Exited']

y



"""Split the dataset for train and test"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.20, random_state=42)

print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

"""Model Fitting"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()

#fit the model
lr.fit(x_train, y_train)

# accuracy libraries
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# Training accuracy
train_error = lr.predict(x_train)

# see accuracy score
accuracy = accuracy_score(y_train, train_error) * 100
print(accuracy)

# Training confusion metrics
cm = confusion_matrix(y_train, train_error)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')

report = classification_report(y_train, train_error)

print(report)

# ROC - AUC curve
from sklearn.metrics import roc_auc_score
auc_score = roc_auc_score(y_train, train_error)
print("ROC-AUC Score:", auc_score)

# visualize curve
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_train, train_error)

# Calculate AUC
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')  # Diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()

# Test error
y_pred = lr.predict(x_test)

# see accuracy score
accuracy = accuracy_score(y_test, y_pred) * 100
print(accuracy)

# confusion metrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d')

cr = classification_report(y_test, y_pred)
print(cr)

# ROC - AUC curve
from sklearn.metrics import roc_auc_score
auc_score = roc_auc_score(y_train, train_error)
print("ROC-AUC Score:", auc_score)

# ROC - AUC curve
from sklearn.metrics import roc_auc_score
auc_score = roc_auc_score(y_test, y_pred)
print(auc_score)

# visualize curve
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

# Calculate AUC
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')  # Diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()

"""**See Each feature importance**

Using SHAP (SHapley Additive exPlanations)

SHAP is great for understanding how each feature impacts individual predictions.
"""

import shap

explainer = shap.Explainer(lr, x_train)
shap_values = explainer(x_train)

shap.summary_plot(shap_values, x_train)  # Summary plot

"""Using Permutation Importance

This method shuffles feature values and measures the drop in model performance.

"""

from sklearn.inspection import permutation_importance

result = permutation_importance(lr, x_test, y_test, scoring='f1')

# Convert results into a DataFrame
perm_importance = pd.DataFrame({'Feature': x_test.columns, 'Importance': result.importances_mean})
perm_importance = perm_importance.sort_values(by='Importance', ascending=False)

# Display feature importance
print(perm_importance.head(10))

"""Other Models - K-NN, SVM, Decision Tree, Random Forest"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

"""**If you want to find best metrics of KNN**"""

from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors
    'weights': ['uniform', 'distance'],  # Weighting function
    'metric': ['euclidean', 'manhattan', 'minkowski']  # Distance metric
}

knn = KNeighborsClassifier()
grid_knn = GridSearchCV(knn, param_grid, cv=5, scoring='f1')
grid_knn.fit(x_train, y_train)

print("Best KNN Parameters:", grid_knn.best_params_)

"""**For SVM**"""

from sklearn.svm import SVC

param_grid = {
    'C': [0.1, 1, 10, 100],  # Regularization parameter
    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Kernel type
    'gamma': ['scale', 'auto']  # Kernel coefficient
}

svm = SVC()
grid_svm = GridSearchCV(svm, param_grid, cv=5, scoring='f1')
grid_svm.fit(x_train, y_train)

print("Best SVM Parameters:", grid_svm.best_params_)

"""**For Decision Tree**"""

from sklearn.tree import DecisionTreeClassifier

param_grid = {
    'criterion': ['gini', 'entropy'],  # Splitting criterion
    'max_depth': [None, 10, 20, 30],  # Tree depth
    'min_samples_split': [2, 5, 10],  # Minimum samples to split a node
    'min_samples_leaf': [1, 2, 4]  # Minimum samples per leaf
}

dt = DecisionTreeClassifier()
grid_dt = GridSearchCV(dt, param_grid, cv=5, scoring='f1')
grid_dt.fit(X_train, y_train)

print("Best Decision Tree Parameters:", grid_dt.best_params_)

"""**For Random Forest**"""

from sklearn.ensemble import RandomForestClassifier

param_grid = {
    'n_estimators': [50, 100, 200],  # Number of trees
    'max_depth': [None, 10, 20, 30],  # Max depth of each tree
    'min_samples_split': [2, 5, 10],  # Minimum samples to split
    'min_samples_leaf': [1, 2, 4],  # Minimum samples per leaf
    'bootstrap': [True, False]  # Bootstrap sampling
}

rf = RandomForestClassifier()
grid_rf = GridSearchCV(rf, param_grid, cv=5, scoring='f1')
grid_rf.fit(x_train, y_train)

print("Best Random Forest Parameters:", grid_rf.best_params_)

"""**For Gradient boosting**"""

from sklearn.ensemble import GradientBoostingClassifier

param_grid = {
    'n_estimators': [50, 100, 200],  # Number of boosting stages
    'learning_rate': [0.01, 0.1, 0.2],  # Step size shrinkage
    'max_depth': [3, 5, 10],  # Max depth of trees
    'min_samples_split': [2, 5, 10],  # Minimum samples to split
    'min_samples_leaf': [1, 2, 4]  # Minimum samples per leaf
}

gb = GradientBoostingClassifier()
grid_gb = GridSearchCV(gb, param_grid, cv=5, scoring='f1')
grid_gb.fit(X_train, y_train)

print("Best Gradient Boosting Parameters:", grid_gb.best_params_)

"""**we can do that or we can define**"""

a = KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='auto', metric='minkowski')
b = SVC(kernel='rbf', C=10, gamma='auto')
c = DecisionTreeClassifier(criterion='gini', max_depth=15, min_samples_split=3, min_samples_leaf=1, max_features=None, random_state=42)
d = RandomForestClassifier(n_estimators=400, criterion='entropy', max_depth=15, min_samples_split=2, min_samples_leaf=1, bootstrap=True, random_state=42)
e = GradientBoostingClassifier(n_estimators=400, learning_rate=0.05, max_depth=3, min_samples_split=2, min_samples_leaf=1)

models = [a,b,c,d,e]
model_name = ['KNN-Classifier', 'SVC-Classifier', 'Decision Tree', 'Random Forest', 'Gradient Boosting Classification']

# store results
results = []

import seaborn as sns
import matplotlib.pyplot as plt
# Run all models at once
for model, names in zip(models, model_name):
  #Fit the model
  model.fit(x_train, y_train)

  #Make predictions
  training_error = model.predict(x_train)
  testing_error = model.predict(x_test)

  #Calculate Training error
  train_accuracy = accuracy_score(y_train, training_error)
  train_cm = confusion_matrix(y_train, training_error)
  train_cr = classification_report(y_train, training_error)
  #Visualize confusion metrics
  sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues')
  plt.title("Training Confusion Metrics")
  plt.show()

  #Calculate Test error
  test_accuracy = accuracy_score(y_test, testing_error)
  test_cm = confusion_matrix(y_test, testing_error)
  test_cr = classification_report(y_test, testing_error)
  #visualize confusion metrics
  sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues')
  plt.title("Testing Confusion Metrics")
  plt.show()

  #print results
  print(f'Model name -- {names}')
  print(f'Training results -- Accuracy={train_accuracy}')
  print(f'Confusion metrics=\n{train_cm}')
  print(f'Classification report=\n{train_cr}')
  print("-" * 100)
  print(f'Testing results -- Accuracy={test_accuracy}')
  print(f'Confusion metrics=\n{test_cm}')
  print(f'Classification report=\n{test_cr}')
  print("||" * 100)

# Store results in the list
results.append({
        'Model': name,
        'Train Accuracy': train_accuracy,
        'Train Confusion Matrix': train_cm.tolist(),  # Convert to list for better readability
        'Train Classification Report': train_cr,
        'Test Accuracy': test_accuracy,
        'Test Confusion Matrix': test_cm.tolist(),  # Convert to list for better readability
        'Test Classification Report': test_cr
    })

# Convert results to a DataFrame for easier handling
results_df = pd.DataFrame(results)

print(results_df)

"""Trying cross - validation on KNN"""

from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import make_classification

# Assuming you have your features X and target y
# X, y = your dataset features and target

# Create KNN classifier with best parameters
knn = KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='auto', metric='minkowski')

# Apply cross-validation
cv_scores = cross_val_score(knn, x, y, cv=5)  # You can change cv to the number of folds you want

# Print the cross-validation scores
print("Cross-validation scores:", cv_scores)
print("Mean cross-validation score:", cv_scores.mean())

"""Trying cross validaiton on random forest"""

Random = RandomForestClassifier(n_estimators=400, criterion='entropy', max_depth=20, bootstrap=True, random_state=42)

rcv = cross_val_score(Random, x, y, cv=5)

print(cv_scores)
print(cv_scores.mean())

